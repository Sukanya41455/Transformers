{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers with tf serving.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4byKFL7PDVF"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq grpcio==1.26.0 transformers tensorflow_serving_api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_serving.apis import predict_pb2\n",
        "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
        "import grpc\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizerFast, BertConfig"
      ],
      "metadata": {
        "id": "kvn5p3nBPHjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"nateraw/bert-base-uncased-imdb\", from_pt=True)\n",
        "# the saved_model parameter is a flag to create a saved model version of the model in same time than the h5 weights\n",
        "model.save_pretrained(MODEL_DIR, saved_model=True)\n",
        "os.environ[\"MODEL_DIR\"] = os.path.join(MODEL_DIR, \"saved_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HECO9H9uPXqw",
        "outputId": "ecced393-5ce3-423f-cf5b-02e6c32f5024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/saved_model/1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/saved_model/1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if saved model is properly formatted\n",
        "!saved_model_cli show --dir {MODEL_DIR}/saved_model/1 --tag_set serve --signature_def serving_default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0s6uLG5PjlA",
        "outputId": "acd7ede2-5267-452c-d18f-780341b76a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['attention_mask'] tensor_info:\n",
            "      dtype: DT_INT32\n",
            "      shape: (-1, -1)\n",
            "      name: serving_default_attention_mask:0\n",
            "  inputs['input_ids'] tensor_info:\n",
            "      dtype: DT_INT32\n",
            "      shape: (-1, -1)\n",
            "      name: serving_default_input_ids:0\n",
            "  inputs['token_type_ids'] tensor_info:\n",
            "      dtype: DT_INT32\n",
            "      shape: (-1, -1)\n",
            "      name: serving_default_token_type_ids:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['logits'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 2)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install apt package for tf serving\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update\n",
        "!apt-get install tensorflow-model-server"
      ],
      "metadata": {
        "id": "gzl0l_2WPrrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a tf serving\n",
        "%%bash --bg\n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --grpc_api_port=8500 \\\n",
        "  --model_name=bert \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqulUNufP3mW",
        "outputId": "ac9c6cf8-d99b-49e4-c2fe-e647ec6e5a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job # 5 in a separate thread.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if server runs properly\n",
        "!tail server.log"
      ],
      "metadata": {
        "id": "clAa1RFHQcPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the requirements for the tests\n",
        "sentence = \"I love the new TensorFlow update in transformers.\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"nateraw/bert-base-uncased-imdb\")\n",
        "config = BertConfig.from_pretrained(\"nateraw/bert-base-uncased-imdb\")"
      ],
      "metadata": {
        "id": "Wi9coh_XQgbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference over REST API\n",
        "batch = tokenizer(sentence)\n",
        "batch = dict(batch)\n",
        "batch = [batch]\n",
        "\n",
        "input_data = {\"instance\": batch}\n",
        "r = requests.post(\"https://localhost:8501/v1/models/bert:predict\", data=json.dumps(input_data))\n",
        "result = json.loads(r.text)['predictions'][0]\n",
        "abs_scores = np.abs(result)\n",
        "label_id = np.argmax(abs_scores)\n",
        "print(config.id2[label_id])"
      ],
      "metadata": {
        "id": "Q1J95IDxRcv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an inference over gRPC API\n",
        "# Tokenize the sentence but this time with TensorFlow tensors as output already batch sized to 1. Ex:\n",
        "# {\n",
        "#    'input_ids': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[  101, 19082,   102]])>,\n",
        "#    'token_type_ids': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[0, 0, 0]])>,\n",
        "#    'attention_mask': <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 1, 1]])>\n",
        "# }\n",
        "batch = tokenizer(sentence, return_tensors='tf')\n",
        "channel = grpc.insecure_channel('localhost:8502')\n",
        "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "request = predict_pb2.PredictRequest()\n",
        "request.model_spec.name = 'bert'\n",
        "request.model_spec.signature_name = 'serving_default'\n",
        "request.inputs['input_ids'].CopyFrom(tf.make_tensor_proto(batch['input_ids']))\n",
        "request.inputs[\"attention_mask\"].CopyFrom(tf.make_tensor_proto(batch[\"attention_mask\"]))\n",
        "request.inputs[\"token_type_ids\"].CopyFrom(tf.make_tensor_proto(batch[\"token_type_ids\"]))\n",
        "\n",
        "result = stub.Predict(request)\n",
        "\n",
        "output = result.outputs['logits'].float_val\n",
        "print(config.id2label[np.argmax(np.abs(output))])"
      ],
      "metadata": {
        "id": "aUYy9f-zSNrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SoWveYwWTvZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}